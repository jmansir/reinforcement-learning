{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>__Define Classes__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first step is to define a `KArmedBandit` class. It needs to keep track of:\n",
    "### 1] How many arms it has, $k$\n",
    "\n",
    "### 2] The mean reward $\\mu_a$ for pulling each arm \n",
    "><font size=3>_Begin by choosing these means from a normal distribution._</font>\n",
    "### 3] The standard deviation $\\sigma_a$ of rewards for pulling each arm\n",
    "><font size=3>_To begin with, this can be set to 1 for all arms, but feel free to experiment later!_</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `KArmedBandit` should also have:\n",
    "### 1] A function that pulls a specified lever and returns a reward drawn from the correct distribution\n",
    "### 2] A function that returns the index of the optimal arm to pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedBandit():\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        # self.means = ?\n",
    "        # self.stdevs = ?\n",
    "    \n",
    "    def pull_lever(self, arm):\n",
    "        pass\n",
    "        #return ?\n",
    "        \n",
    "    def optimal_arm(self):\n",
    "        pass\n",
    "        #return ?\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, define an `Agent` class. The agent should keep track of:\n",
    "### 1] Its exploration rate $\\epsilon$\n",
    "### 2] Its current estimate of the true action value function  $Q_t(s, a)$\n",
    "### 3] The number of times it has pulled each lever, $n_a$\n",
    "### 4] The number of times it has pulled the optimal lever\n",
    "### 5] A record of rewards received per time step\n",
    "### 6] A record of the percentage of total pulls that were optimal as a function of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `Agent` should also have:\n",
    "\n",
    "### 1] A function to choose a lever $\\epsilon$-greedily\n",
    "### 2] An `act` function, which pulls a lever, receives a reward, and updates tracking of rewards and optimal pull %\n",
    "### 3] An `update_Q` function, which updates our estimated action-value function $Q(s,a)$ given a reward and which lever was pulled\n",
    "><font size=4>_The simplest way to do this is to keep track of the rewards assigned to each lever, but there is a more elegant solution_</font>\n",
    "### 4] A `run_trial` function, which performs `act` $n_{steps}$ times\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agent should have an act function, which chooses an action e-greedily, receives a reward, and updates the reward and optimal pull % trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agent should also have a function to update its action-value function Q_t upon pulling a lever and receiving a reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, bandit, epsilon):\n",
    "        self.bandit = bandit\n",
    "        self.epsilon = epsilon\n",
    "        self.num_optimal_pulls = 0\n",
    "        self.reward_trajectory = []\n",
    "        self.optimal_trajectory = []\n",
    "        self.n = [0]*self.bandit.k\n",
    "        \n",
    "        # How will you initialise your Q estimates?\n",
    "        # self.Q = ?\n",
    "        \n",
    "        \n",
    "    \n",
    "    def choose_e_greedy_action(self):\n",
    "        pass\n",
    "        # selected_lever = ?\n",
    "        # return selected_lever\n",
    "    \n",
    "    def act(self):\n",
    "        \n",
    "        # Choose an action e-greedily\n",
    "        lever = self.choose_e_greedy_action()\n",
    "        \n",
    "        # Update the array keeping track of how many times each lever has been pulled\n",
    "        self.n[lever] += 1\n",
    "        \n",
    "        # Now that you know which lever to pull, how will you get the reward?\n",
    "        # reward = ?\n",
    "        \n",
    "        # Did the agent pull the optimal arm?\n",
    "        if lever == self.bandit.optimal_arm():\n",
    "            self.num_optimal_pulls += 1\n",
    "        \n",
    "        # Update your reward and optimal pull % trajectories\n",
    "        # self.reward_trajectory ?\n",
    "        # self.optimal_trajectory ?\n",
    "\n",
    "        # Update your q estimate\n",
    "        self.update_Q(?)\n",
    "    \n",
    "    # What arguments does this function need to take?\n",
    "    # How will you update the Q values?\n",
    "    def update_Q(self, lever, reward):\n",
    "        pass\n",
    "\n",
    "    def run_trial(self, n_steps):\n",
    "        for step in range(n_steps):\n",
    "            # Which function needs to be called here?\n",
    "            # self.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "num_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Bandit & Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = KArmedBandit(k = k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(bandit, epsilon = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a single trial and plot the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run_trial(num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(agent.reward_trajectory)\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel('Steps', fontsize=16)\n",
    "ax.set_ylabel('Reward received', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(agent.optimal_trajectory)\n",
    "ax.tick_params(labelsize=16)\n",
    "ax.set_xlabel('Steps', fontsize=16)\n",
    "ax.set_ylabel('% of time we pulled the optimal lever', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the bandit means to your best estimates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit.means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Trials of different epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 1000\n",
    "epsilons_to_test = [0.01, 0.1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward_trajectory_array = []\n",
    "mean_optimal_trajectory_array = []\n",
    "\n",
    "for eps in epsilons_to_test:\n",
    "    \n",
    "    print(f'Testing epsilon = {eps}')\n",
    "    # Initialise containers for trajectories\n",
    "    reward_trajectory_array = []\n",
    "    optimal_trajectory_array = []\n",
    "    \n",
    "    # Run num_trials trials and average the results\n",
    "    for trial in tqdm_notebook(range(num_trials)):\n",
    "        bandit = KArmedBandit(k = k)\n",
    "        ag = Agent(bandit, eps)\n",
    "    \n",
    "        ag.run_trial(num_steps)\n",
    "            \n",
    "        # After each trial, add the reward and optimal % trajectory to an array     \n",
    "        reward_trajectory_array.append(ag.reward_trajectory)\n",
    "        optimal_trajectory_array.append(ag.optimal_trajectory)\n",
    "    \n",
    "    # After running num_trials trials, take the mean of the trajectories and store them\n",
    "    # in an array\n",
    "    mean_reward_trajectory_array.append(np.mean(np.array(reward_trajectory_array), axis=0))\n",
    "    mean_optimal_trajectory_array.append(np.mean(np.array(optimal_trajectory_array), axis=0))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "for idx, eps in enumerate(epsilons_to_test):\n",
    "    ax.plot(mean_reward_trajectory_array[idx])\n",
    "ax.legend([str(e) for e in epsilons_to_test],fontsize=16)\n",
    "ax.set_xlabel('Steps', fontsize=16)\n",
    "ax.set_ylabel('Mean reward', fontsize=16)\n",
    "ax.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "for idx, eps in enumerate(epsilons_to_test):\n",
    "    ax.plot(mean_optimal_trajectory_array[idx])\n",
    "ax.legend([str(e) for e in epsilons_to_test], fontsize=16)\n",
    "ax.set_xlabel('Steps', fontsize=16)\n",
    "ax.set_ylabel('Optimal lever %', fontsize=16)\n",
    "ax.tick_params(labelsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How do your estimates Q_t of the true action-value function q* converge over time? As a function of epsilon?\n",
    "\n",
    "### 2. Optimistic initialisation: what happens when you initialise your Q values at 5 instead of 0? why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Non-stationary q*(a): try adding a function that slightly modifies your bandit means after every time the agent acts. What happens now? (Hint: your Q_n update function should value nearer rewards to those further away. How can we achieve this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
