{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q learning taxi game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size =4> Be sure to install the OpenAI Gym, by running this from the terminal: \n",
    " - `pip install gym`\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Open the Taxi game from OpenAI Gym</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Note that there are 500 possible environment states</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Reset the environment to a random state</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset() # This is a useful function - you will need it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Here is what the environmental states look like...\n",
    " - The yellow rectangle is the taxi\n",
    " - The |'s are walls\n",
    " - The :'s are road\n",
    " - The goal is to pick up someone at blue letter, then drop off at pink letter</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Note that there are 6 possible actions the taxi can take at any time step: \n",
    " - down, up, right, left, pick up, drop off</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Experiment with taking actions...\n",
    "    \n",
    "- The argument to the step( ) function is the index of the action you want to take\n",
    "\n",
    "- The state returned by the step( ) function is the index of the new state we are in after performing that action.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The argument to this function is the action.\n",
    "# 0: down, 1: up, 2: left, 3: right, 4: pickup, 5: dropoff\n",
    "new_state, reward, done, info = env.step(5) # You will also need this function!\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform tabular Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Initialize the action-value function Q(state, action) and choose a learning rate $\\alpha$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise your Q(s,a) table. What shape should the table be?\n",
    "# You can find the shape of the state space with env.observation_space.n\n",
    "# The action space is env.action_space. Find its shape also\n",
    "# Try initialising all values to 0 at first, or play with random initialisation\n",
    "\n",
    "\n",
    "# Q = ?\n",
    "\n",
    "\n",
    "# Choose a sensible learning rate\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Play the game for a certain number of episodes, updating the Q-function after each action</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Choose a number of episodes to run\n",
    "n_episodes = 100000\n",
    "print_freq = 10 # logarithmic\n",
    "prev_freq = 0\n",
    "\n",
    "avg_step_count = 0\n",
    "for episode in range(1, n_episodes+1):\n",
    "    \n",
    "    # Initialise the state.\n",
    "    # state = ?\n",
    "    \n",
    "    \n",
    "    # Need to keep track of the return for each episode.\n",
    "    # Initialise a variable to keep track of this\n",
    "    G = 0\n",
    "    \n",
    "    step_count = 0\n",
    "    \n",
    "    # Let epsilon-greediness decay with episode. Try experimenting with this!\n",
    "    epsilon = 10 / episode\n",
    "    \n",
    "    # Continue until taxi performs the correct pick-up and drop-off (thus earning 20 points)\n",
    "    reward = 0\n",
    "    while reward != 20:\n",
    "        \n",
    "        # Choose action epsilon-greedily\n",
    "        # action = ?\n",
    "\n",
    "        # Perform the action you chose, and receive the new state and reward\n",
    "        # what function should go here?\n",
    "        \n",
    "        step_count += 1\n",
    "        G += reward\n",
    "        \n",
    "        # Update action-value function according to Q-learning algorithm\n",
    "        # Refer to slide for update equation!\n",
    "        \n",
    "        # Update state and proceed to next action\n",
    "        state = new_state\n",
    "        \n",
    "    # Track stats\n",
    "    # This will print out how many steps on average it takes your q-learning\n",
    "    # agent to complete the task.\n",
    "    if episode % print_freq == 0:\n",
    "        avg_step_count += 1/print_freq * (step_count - avg_step_count)\n",
    "        print('Episode: {}, Average Step Count: {:.2f}'.format(episode, avg_step_count))\n",
    "        avg_step_count = 0\n",
    "        prev_freq = print_freq\n",
    "        print_freq *= 10\n",
    "    else:\n",
    "        avg_step_count += 1/(episode - prev_freq) * (step_count - avg_step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used for interactive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(Q, epsilon):\n",
    "    states = []\n",
    "    actions = []\n",
    "    state = env.reset()\n",
    "    states.append(state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "        actions.append(action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        states.append(state)\n",
    "    return states, actions\n",
    "\n",
    "def snapshot(t):\n",
    "    if t == 0:\n",
    "        env.reset()\n",
    "        env.env.s = states[0]\n",
    "    else:\n",
    "        env.env.s = states[t-1]\n",
    "        env.step(actions[t-1])\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the initial random strategy in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions = taxi_functions.run_episode(Q, 1)\n",
    "taxi_functions.interact(snapshot, t=widgets.IntSlider(min=0, max=len(states)-1, step=1, value=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the learned optimal policy in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions = run_episode(Q, 0)\n",
    "interact(snapshot, t=widgets.IntSlider(min=0, max=len(states)-1, step=1, value=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens to the convergence rate if you randomly initialise your Q table instead of setting it to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try repeating this for a different game in the OpenAI gym! https://gym.openai.com/envs/FrozenLake8x8-v0/ is a good choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
