{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q learning taxi game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to install the OpenAI Gym, by running this from the terminal: \n",
    " - pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import ipywidgets as widgets\n",
    "import taxi_functions\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the Taxi game from OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the environment to a random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 500 possible environment states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what they look like...\n",
    " - The yellow rectangle is the taxi\n",
    " - The |'s are walls\n",
    " - The :'s are road\n",
    " - The goal is to pick up someone at blue letter, then drop off at pink letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 6 possible actions the taxi can take at any time step: \n",
    " - down, up, right, left, pick up, drop off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with taking actions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform tabular Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For background:\n",
    "\n",
    " - The action-value function Q(state, action) is defined in Equation (4) of the notes.\n",
    "\n",
    " - The Q-learning algorithm is covered in Section 5.2 of the notes; see Equation (46)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the action-value function Q(state, action) and choose a learning rate alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the game for a certain number of episodes, updating the Q-function after each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_episodes = 10**5\n",
    "print_freq = 10 # logarithmic\n",
    "prev_freq = 0\n",
    "\n",
    "avg_step_count = 0\n",
    "for episode in range(1, n_episodes+1):\n",
    "    state = env.reset()\n",
    "    G = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    # Let epsilon-greediness decay with episode\n",
    "    epsilon = 10 / episode\n",
    "    \n",
    "    # Continue until taxi stumbles upon correct pick-up and drop-off (thus earning 20 points)\n",
    "    reward = 0\n",
    "    while reward != 20:\n",
    "        \n",
    "        # Choose action epsilon-greedily\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "        \n",
    "        # Perform action\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        step_count += 1\n",
    "        G += reward\n",
    "        \n",
    "        # Update action-value function according to Q-learning algorithm\n",
    "        Q[state, action] += alpha * (reward + np.max(Q[state2]) - Q[state, action])\n",
    "        \n",
    "        # Update state and proceed to next action\n",
    "        state = state2\n",
    "        \n",
    "    # Track stats\n",
    "    if episode % print_freq == 0:\n",
    "        avg_step_count += 1/print_freq * (step_count - avg_step_count)\n",
    "        print('Episode: {}, Average Step Count: {:.2f}'.format(episode, avg_step_count))\n",
    "        avg_step_count = 0\n",
    "        prev_freq = print_freq\n",
    "        print_freq *= 10\n",
    "    else:\n",
    "        avg_step_count += 1/(episode - prev_freq) * (step_count - avg_step_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used for interactive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(Q, epsilon):\n",
    "    states = []\n",
    "    actions = []\n",
    "    state = env.reset()\n",
    "    states.append(state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "        actions.append(action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        states.append(state)\n",
    "    return states, actions\n",
    "\n",
    "def snapshot(t):\n",
    "    if t == 0:\n",
    "        env.reset()\n",
    "        env.env.s = states[0]\n",
    "    else:\n",
    "        env.env.s = states[t-1]\n",
    "        env.step(actions[t-1])\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of the initial random strategy in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions = run_episode(Q, 1)\n",
    "interact(snapshot, t=widgets.IntSlider(min=0, max=len(states)-1, step=1, value=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the learned optimal policy in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions = run_episode(Q, 0)\n",
    "interact(snapshot, t=widgets.IntSlider(min=0, max=len(states)-1, step=1, value=0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
